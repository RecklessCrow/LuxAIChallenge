import os
from datetime import datetime

import numpy as np

from luxai2021.game.constants import Constants, LuxMatchConfigs_Default
from luxai2021.game.game_constants import GAME_CONSTANTS

CONFIGS = LuxMatchConfigs_Default

RESOURCE_LIST = [Constants.RESOURCE_TYPES.WOOD, Constants.RESOURCE_TYPES.COAL, Constants.RESOURCE_TYPES.URANIUM]

MAX_DAYS = GAME_CONSTANTS["PARAMETERS"]["MAX_DAYS"]

NUM_OBSERVATIONS = 5
NUM_RESOURCE_OBSERVATIONS = 10

GAME_STATE_CATEGORIES = ['is_night', '%_of_cycle_passed', '%_game_complete', 'coal_research_progress', 'uranium_research_progress', 'worker_cap_reached', 'team']
GAME_STATE_LEN = len(GAME_STATE_CATEGORIES)
GAME_STATE_IDX_DICT = dict(zip(GAME_STATE_CATEGORIES, np.arange(0, len(GAME_STATE_CATEGORIES))))

UNIT_CATEGORIES = ['team', 'city', 'worker', 'cart', 'inventory', 'x', 'y']
UNIT_LEN = len(UNIT_CATEGORIES)
UNIT_IDX_DICT = dict(zip(UNIT_CATEGORIES, np.arange(0, len(UNIT_CATEGORIES))))

RESOURCE_CATEGORIES = ['wood', 'coal', 'uranium', 'amount', 'x', 'y']
RESOURCE_LEN = len(RESOURCE_CATEGORIES)
RESOURCE_IDX_DICT = dict(zip(RESOURCE_CATEGORIES, np.arange(0, RESOURCE_LEN)))

OBSERVATION_SHAPE = (len(GAME_STATE_CATEGORIES) + (len(UNIT_CATEGORIES) * (NUM_OBSERVATIONS * 4 + 1)) + (len(RESOURCE_CATEGORIES) * NUM_RESOURCE_OBSERVATIONS), )

# Observation Constants
RESERACH_FOR_COAL = 50
MAX_RESEARCH = 200
MAX_UNIT_COUNT = 30
MAX_CITY_COUNT = 30
STARTING_CITIES = 1
STARTING_UNITS = 1
TEAMS = [0, 1]

NUM_STEPS_IN_DAY = 30
NUM_STEPS_IN_NIGHT = 10

# Reward Constants
MAX_REWARD = 1
MIN_REWARD = -MAX_REWARD

FUEL_DEPOSITED_REWARD_MODIFIER = 0.001
WOOD_GATHERED_REWARD_MODIFIER = 0.0001
COAL_GATHERED_REWARD_MODIFIER = WOOD_GATHERED_REWARD_MODIFIER * 10
URANIUM_GATHERED_REWARD_MODIFIER = WOOD_GATHERED_REWARD_MODIFIER * 40

RESEARCH_REWARD_MODIFIER = 0.00001
RESEARCH_GOAL_MET_MODIFIER = 0.1
COAL_UNLOCKED = MAX_REWARD
URANIUM_UNLOCKED = MAX_REWARD

GAME_WIN = MAX_REWARD
GAME_LOSS = -GAME_WIN

# Unused
# CITY_REWARD_MODIFIER = 0.5
# NEGATIVE_CITY_MODIFIER = 1.25
# UNIT_REWARD_MODIFIER = 0.1
# NEGATIVE_UNIT_MODIFIER = 1.25
# CITY_STANDING_REWARD_MODIFIER = CITY_REWARD_MODIFIER * 2

# Hyper Parameters
LEARNING_RATE = 3e-4
GAMMA = 0.995
GAE_LAMBDA = 0.95
BATCH_SIZE = 4096
TRAINING_STEPS = 10_000_000
NUM_STEPS = BATCH_SIZE

# Multiprocessing
NUM_EVAL_ENVS = 4
NUM_EVAL_GAMES = 5
NUM_ENVS = 1  # os.cpu_count()

# Logging
NUM_REPLAYS = 10
SAVE_FREQ = (TRAINING_STEPS // NUM_REPLAYS) // NUM_ENVS

TIME_STAMP = datetime.now().strftime('%m-%d_%H-%M-%S')

CHECKPOINT_PATH = os.path.join("..", "checkpoints")
MODEL_PATH = os.path.join("..", 'models')
if not os.path.exists(CHECKPOINT_PATH):
    os.mkdir(CHECKPOINT_PATH)

LOGS_PATH = os.path.join("..", "logs")
CALLBACKS_PATH = os.path.join(LOGS_PATH, f"{TIME_STAMP}")

MODEL_CHECKPOINT_PATH = os.path.join(CHECKPOINT_PATH, TIME_STAMP)
MODEL_PATH = os.path.join("..", "models", f"{TIME_STAMP}.zip")

SAVED_MODEL_PATH = os.path.join(MODEL_PATH, ".zip")
